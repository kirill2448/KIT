# -*- coding: utf-8 -*-
"""Detect_KIT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NWlNxAdR46fmbuaTrQFhVKRMd_xVrdSx
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U -q PyDrive ## you will have install for every colab session

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

     # 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

!wget http://dataudt.ru/datasets/cv/Lesson_04.Train_yolov3.zip

!unzip -qq Lesson_04.Train_yolov3.zip

!rm Lesson_04.Train_yolov3.zip

!pip install -r /content/Lesson_04.Train_yolov3/requirements.txt

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
       print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

!mkdir /content/trainsdata

!mkdir /content/testsdata

z='/content/trainsdata'
v='/content/testsdata'

from google.colab import files

v= files.upload()

for fn in uploaded.keys():
       print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

!ls /content/Lesson_04.Train_yolov3/Datasets/fire_dataset/train

l=0
for i in range(138):
  print(str('/content/Lesson_04.Train_yolov3/Datasets/fire_dataset/test/'),l+i,str('.jpg'))

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/Images/
# %ls

# Commented out IPython magic to ensure Python compatibility.
!wget https://pjreddie.com/media/files/yolov3.weights
# %ls



# Основные модули.
import numpy as np
import cv2
import tensorflow as tf

# Функции для конструирования сети.
from tensorflow.keras.backend import clear_session
from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization, MaxPool2D
from tensorflow.keras.regularizers import l2

# Функция для отображения изображений.
from google.colab.patches import cv2_imshow
# Модуль для преобразования между цветовыми пространствами.
import colorsys
# Генератор случайных чисел.
import random
# Функции для получения списка файлов в директории.
from os import listdir
from os.path import isfile, join

# Базовые модули.
import os
import shutil
import cv2
import random
import numpy as np
import tensorflow as tf

# Функция для отображений изображений.
from google.colab.patches import cv2_imshow
# Функция для очистки сессии.
from tensorflow.keras.backend import clear_session
# Функция для загрузки модели.
from tensorflow.keras.models import load_model

!ls

# Зададим размер входа сети.
yolo_input_size = 416
# Укажем путь, где хранятся счанные веса модели.
yolo_weights_path = "/content/yolov3.weights"

# Создадим список классов.
class_names_list = ["1", "2", "3", "4", "5", "6", "7", "8",
	"9", "10", "11", "12", "13", "14",
	"15", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
	"backpack", "umbrella", "16", "tie", "suitcase", "frisbee", "skis", "snowboard",
	"sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "5",
	"tennis racket", "25", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
	"apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
	"chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
	"remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
	"book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]

# Посчитаем количество классов.
num_classes = len(class_names_list)
print('Total number of classses:', num_classes)

# Создадим словарь индексов и имён классов.
# И отобразим его.
class_names_dict = {}
for id, name in enumerate(class_names_list):
    class_names_dict [id] = name
    print('{}: {}, '.format(id, name), end='')
    if id % 5 == 0 and id != 0:
        print()
print('\b\b')

# Список шагов для каждого выхода сети.
# Размер в пикселях одной ячейки сетки 
# изображения в зависимости от выхода сети.
yolo_strides = [8, 16, 32]
# 416 / 52 = 8 пикселей 
# 416 / 26 = 16 пикселей
# 416 / 13 = 32 пикселя

# Количество якорей на каждый выход сети 
yolo_anchors_per_scale = 3

# Список якорей для каждого выхода сети.
yolo_anchors = [[[10,  13], [16,   30], [33,   23]],
                [[30,  61], [62,   45], [59,  119]],
                [[116, 90], [156, 198], [373, 326]]]

# Преобразуем списки якорей и шагов в удобную форму для использования.
yolo_strides = np.array(yolo_strides)
yolo_anchors = (np.array(yolo_anchors).T/yolo_strides).T

# Переопределим класс пакетной нормализации.
class BatchNormalization(BatchNormalization):
    # В TensorFlow 2.0 есть особенности заморозки слоя BatchNormalization.
    # Пакетная нормализация работает по-разному во время обучения 
    # и во время вывода (predict, evaluate).
    # Чтобы не изменять внутренние обученные переменные 
    # добавим возможность заморозки значений слоя.
    def call(self, x, training=False):
        if not training:
            training = tf.constant(False)
        training = tf.logical_and(training, self.trainable)
        return super().call(x, training)

# Зададим функцию для увеличение разрешения.
def upsample(input_layer):
    # Нам нужно будет увеличивать выход слоя сети, 
    # растягивать в 2 раза по ширине и высоте.
    # Для этого будем использовать интерполяцию методом ближайшего соседа.
    # Это самый простой и быстрый метод.
    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')

# Зададим функцию для определения парамметров свёрточного слоя.
def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):
    # Если уменьшение разрешения активно
    if downsample:
        # заполняем нулями сверху и слева
        # ((top_pad, bottom_pad), (left_pad, right_pad))
        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)
        padding = 'valid' # без заполнения
        strides = 2 # шаг свёртки
    else:
        strides = 1 # шаг свёртки
        padding = 'same' # заполнение до такого же размера

    # Определяем параметры свёртки.
    # filters - количество фильтров
    # kernel_size - размер ядра свёртки
    # strides - шаг ядра свёртки
    # padding - заполнение
    # use_bias - использование смещения
    # kernel_regularizer - регуляризация весов
    # kernel_initializer - инициализация весов
    # bias_initializer - инициализация весов смещения
    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,
                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),
                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),
                  bias_initializer=tf.constant_initializer(0.))(input_layer)
    
    # Если активна пакетная нормализация.
    if bn:
        conv = BatchNormalization()(conv)

    # Если нужно использовать функцию активации.
    if activate == True:
        # используется ReLU с "утечкой"
        conv = LeakyReLU(alpha=0.1)(conv)

    # Возвращаем определённый свёрточный слой.
    return conv

# Зададим функцию для определения блока короткого соединения.
def residual_block(input_layer, input_channel, filter_num1, filter_num2):
    # Сохраняем значения короткого соединения на входе в блок
    short_cut = input_layer

    # Пропускаем через слои свёртки.
    # Cвёртка 1 х 1 для уменьшения размерности 
    # (свёртка 1 х 1 объединяет все каналы входных пикселей в один пиксель).
    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))
    # свёртка 3 х 3 
    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))

    # Отстаточные или короткие соединения нужны для того,
    # чтобы избавится от проблемы затухания градиента
    # в глубоких сетях с большим количеством слоёв

    # Складываем значения короткого соединения с выходными значениями свёртки
    residual_output = short_cut + conv

    # Возвращаем полученный результат
    return residual_output

# Зададим архитектуру сети Darknet-53.
def darknet53(input_data):
    # свёртка
    input_data = convolutional(input_data, (3, 3,  3,  32))
    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)

    # Остаточный блок - это блок свёртки с
    # коротким прямым соединением входа с выходом.
    for i in range(1):
        input_data = residual_block(input_data,  64,  32, 64)

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)

    # остаточные блоки
    for i in range(2):
        input_data = residual_block(input_data, 128,  64, 128)

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)

    # остаточные блоки
    for i in range(8):
        input_data = residual_block(input_data, 256, 128, 256)

    # сохраняем ответвеление маршрута
    route_1 = input_data

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)

    # остаточные блоки
    for i in range(8):
        input_data = residual_block(input_data, 512, 256, 512)

    # сохраняем ответвеление маршрута
    route_2 = input_data

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)

    # остаточные блоки
    for i in range(4):
        input_data = residual_block(input_data, 1024, 512, 1024)

    # возращаем ответвления и выход сети 
    return route_1, route_2, input_data

# Зададим архитектуру сети YOLOv3.
def YOLOv3(input_layer):
    # В качестве входного блока используется сеть Darknet-53.
    # На выходе сети имеем два ответвления и выход сети
    route_1, route_2, conv = darknet53(input_layer)

    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3,  512, 1024))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3,  512, 1024))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))
    
    # Первый выход сети для предсказания объектов
    # большого размера, имеет форму [None, 13, 13, 255]. 
    # 255 получается если кол-во классов (80 + 5) * 3.
    # 5 значений - это координаты центра объекта x и y,
    # ширина и высота ограничивающей рамки w и h,
    # и показатель уверенности что в ограничивающей рамке есть объект.
    # 3 - это количество якорей ограничивающих рамок.
    # 13 x 13 - это размер сетки для больших объектов,
    # которой делится входное изображение.
    # Большие объекты -> меньше делений сетки.
    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(num_classes+5)), activate=False, bn=False)

    # свёртка 1 х 1 с основным маршрутом сети
    conv = convolutional(conv, (1, 1,  512,  256))

    # Растягиваем текущий выход сети для совпадения размеров
    # со следующим слоем
    conv = upsample(conv)

    # Присоединяем текущий маршрут с ответвлением сети Darknet-53
    # вдоль последней оси
    conv = tf.concat([conv, route_2], axis=-1)
    
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 768, 256))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 256, 512))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 512, 256))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 256, 512))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 512, 256))
    # свёртка 3 х 3
    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))

    # Второй выход сети для предсказания объектов
    # среднего размера, имеет форму [None, 26, 26, 255] 
    # 26 x 26 - это размер сетки для средних объектов.
    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(num_classes + 5)), activate=False, bn=False)

    # свёртка 1 х 1 с основным маршрутом сети
    conv = convolutional(conv, (1, 1, 256, 128))

    # Растягиваем текущий выход сети для совпадения размеров
    # со следующим слоем
    conv = upsample(conv)

    # Присоединяем текущий маршрут с ответвлением сети Darknet-53
    # вдоль последней оси
    conv = tf.concat([conv, route_1], axis=-1)

    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 384, 128))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 128, 256))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 256, 128))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 128, 256))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 256, 128))
    # свёртка 3 х 3
    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))
    
    # Третий выход сети для предсказания объектов
    # маленького размера, имеет форму [None, 52, 52, 255] 
    # 52 x 52 - это размер сетки для маленьких объектов,
    # которой делится входное изображение.
    # маленькие объекты -> больше делений сетки.
    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(num_classes+5)), activate=False, bn=False)
        
    # Возвращаем список из трёх выходов сети в обратном порядке.
    return [conv_sbbox, conv_mbbox, conv_lbbox]

# Зададим функцию преобразования значений каждого выхода сети к последовательности:
# координаты обекта, ширина и высота ограничивающей рамки, 
# показатель уверенности присутствия объекта и вероятности классов.
def decode(conv_output, i=0):
    # где i = 0, 1 или 2, чтобы соответствовать трем масштабам сетки (выходам сети)
    # форма выхода сети
    conv_shape = tf.shape(conv_output)
    # размер пакета
    batch_size = conv_shape[0]
    # размер выхода сети (13, 26, 52)
    output_size = conv_shape[1]

    # изменяем форму выхода сети, чтобы разделить данные 
    # по трём видам используемых якорей ограничивающих рамок
    # 255 -> (80 + 5) * 3 (как объяснялось выше)
    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + num_classes))

    # Разделяем данные, каждый параметр в свой массив.
    # Смещение центра объекта.
    # Необработанные значения x и y
    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]
    # Предсказанные длинна и ширина ограничивающей рамки
    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]
    # Показатель уверенности что в ограничивающей рамке есть объект
    conv_raw_conf = conv_output[:, :, :, :, 4:5]
    # Веротности присутствия в этой рамке каждого конкретно класса
    conv_raw_prob = conv_output[:, :, :, :, 5: ]

    # Создаём сетку, где output_size равен выходу сети 13, 26, 52.
    # Здесь в комментариях для примера output_size = 13.
    # Создаём вектор в диапазоне от 0 до output_size-1.
    y = tf.range(output_size, dtype=tf.int32) # [0, 1, ... 12]
    # конвертируем в вектор-столбец
    y = tf.expand_dims(y, -1) # [[0], [1], ... [12]]
    # Повторяем вектор output_size раз
    # и получаем матрицу 13 х 13
    # со значениями от 0 до 12 сверху вниз
    y = tf.tile(y, [1, output_size])
    
    # Создаём вектор в диапазоне от 0 до output_size-1
    x = tf.range(output_size, dtype=tf.int32 ) # [0, 1, ... 12]
    # конвертируем в вектор-строку
    x = tf.expand_dims(x, 0) # [[0, 1, ... 12]]
    # Повторяем вектор output_size раз
    # и получаем матрицу 13 х 13
    # со значениями от 0 до 12 слева направо
    x = tf.tile(x, [output_size, 1])

    # Cоздаём в каждой матрице 3 ось
    # и соеденяем 2 матрицы по 3тей оси.
    # Получаем матрицу формы (13, 13, 2),
    # в которой координаты каждого фрагмента сетки.
    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)

    # Преобразовываем сетку к форме (batch_size, 13, 13, 3, 2)
    # чтобы форма совпадала с формой якорей.
    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])
    # Приводим к типу float32.
    xy_grid = tf.cast(xy_grid, tf.float32)

    # Используя формулы с рисунка
    # расчитываем координаты центра объекта.
    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * yolo_strides[i]
    
    # Вычисляем длину и ширину ограничивающей рамки используя якоря.
    pred_wh = (tf.exp(conv_raw_dwdh) * yolo_anchors[i]) * yolo_strides[i]

    # Соединяем вместе значения координат объекта 
    # и ширины, высоты ограничивающей рамки.
    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)

    # Расчитываем уверенность.
    pred_conf = tf.sigmoid(conv_raw_conf)
    # Вычисляем вероятности классов.
    pred_prob = tf.sigmoid(conv_raw_prob)

    # Соединяем все значения вместе и возвращаем.
    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)

# Функция для создания модели Yolo
def Create_Yolo(input_size=416, channels=3):
    # Создаём вход модели с формой 416 x 416 x 3.
    input_layer  = Input([input_size, input_size, channels])

    # Добавляем сеть YOLOv3.
    conv_tensors = YOLOv3(input_layer)

    # Создаём список выходных тензоров.
    output_tensors = []

    # Перебираем каждый выход сети,
    for i, conv_tensor in enumerate(conv_tensors):
        # и декодируем его.
        pred_tensor = decode(conv_tensor, i)
        # Добавляем декодированный выход сети.
        output_tensors.append(pred_tensor)

    # Создаём keras модель и возвращаем её.
    Yolo = tf.keras.Model(input_layer, output_tensors)
    return Yolo

# Функция для загрузки весов в модель.
def load_yolo_weights(model, weights_file):
    # Очищаем сессию чтобы сбросить имена слоёв.
    tf.keras.backend.clear_session()
    # Загружаем оригинальные веса в модель.
    # Yolo имеет 75 свёрточных слоёв.
    range1 = 75 
    # Номера выходных слоёв.
    range2 = [58, 66, 74]   
    
    # Открываем файл с весами модели.
    with open(weights_file, 'rb') as wf:
        # Считываем данные с файла.
        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)

        # Счётчик для слоёв пакетной нормализации.
        j = 0
        # Пербираем слои.
        for i in range(range1):
            
            # Присваиваем имя свёрточному слою и номер по порядку.
            if i > 0:
                conv_layer_name = 'conv2d_%d' %i
            else:
                conv_layer_name = 'conv2d'
                
            # Присваиваем имя слою пакетной нормализации и номер по порядку.
            if j > 0:
                bn_layer_name = 'batch_normalization_%d' %j
            else:
                bn_layer_name = 'batch_normalization'
            
            # Получаем свёрточный слой по имени из модели.
            conv_layer = model.get_layer(conv_layer_name)
            # Из слоя получаем параметры:
            # количество фильтров
            filters = conv_layer.filters
            # размер ядра свёртки
            k_size = conv_layer.kernel_size[0]
            # размер входа слоя
            in_dim = conv_layer.input_shape[-1]

            # Если текущий слой не выходной (используется пакетная нормализация).
            if i not in range2:
                # Считываем с файла парамметры слоя пакетной нормализации.
                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)
                # В фреймворке darknet используется следующий порядок параметров:
                # [beta, gamma, mean, variance].

                # Мы же используем tensorflow, в котором другой
                # порядок следования: [gamma, beta, mean, variance].
                # Изменяем форму и порядок следования.
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]
                
                # Получаем слой пакетной нормализации по имени из модели.
                bn_layer = model.get_layer(bn_layer_name)
                # Увеличиваем счётчик слоёв пакетной нормализации.
                j += 1 
            else:
                # Если слой без батч нотмализации, значит в слое
                # используются нейроны смещения.
                # Считываем значения весов для смещения.
                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)

            # В фреймворке Darknet используется форма
            # свёрточного слоя: (out_dim, in_dim, height, width).
            # В tensorflow: (height, width, in_dim, out_dim)

            # Объявляем кортеж с параметрами формы свёрточного слоя.
            conv_shape = (filters, in_dim, k_size, k_size)

            # Считываем с файла все значения весов слоя.
            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))
            # Изменяем форму и порядок следования.
            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])

            if i not in range2:
                # Если слой не выходной,
                # устанавливаем значения весов в слой модели.
                # Устанавливаем веса свёрточного слоя.
                conv_layer.set_weights([conv_weights])
                # Устанавливаем веса слоя пакетной нормализации.
                bn_layer.set_weights(bn_weights)
            else:
                # Если слой выходной,
                # устанавливаем значения весов свёртки и смещения.
                conv_layer.set_weights([conv_weights, conv_bias])

        # Проверяем пустой ли файл с весами.
        assert len(wf.read()) == 0, 'Failed to read all data'

import requests
import io
import h5py

cd '/content/Lesson_04.Train_yolov3/Model_data/yolov3_fire_weights.h5'

# Функция предобработки входного изображения.
# image - входное изображение.
# target_size - необходимый размер.

# Сеть на вход ожидает квадратное изображение
# размером 416 х 416 х 3.
# Однако размеры и форма входных изображений могут быть другими.
# Для этого изменим размер изображения к необходимому
# не изменяя пропорции. Если изображение прямогуольное,
# то расположим его по центру, а края заполним серым цветом.

def image_preprocess(image, target_size):
    # Размеры, к которым нужно привести изображение.
    ih, iw    = target_size
    # Текущие размеры изображения.
    h,  w, _  = image.shape

    # Расчитываем масштаб изменения размера.
    scale = min(iw/w, ih/h)
    # Умножаем значение ширины и высоты изображения на масштаб.
    nw, nh  = int(scale * w), int(scale * h)
    # Изменяем размер изображения к необходимому.
    image_resized = cv2.resize(image, (nw, nh))

    # Создаём пустое изображение, заполненое серым цветом.
    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)

    # Расчитываем координаты куда копировать масштабированное
    # изображение, чтобы оно было по центру.
    dw, dh = (iw - nw) // 2, (ih-nh) // 2
    
    # Копируем масштабированное изображение в пустое серое.
    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized

    # Полученное изображение переводим в тип данных float32.
    image_paded = image_paded.astype(np.float32)
    
    # Приводим значения интенсивности пикселей 
    # к диапазону от 0 до 1.
    image_paded = image_paded / 255.0

    # Возвращаем преобразованное изображение.
    return image_paded

# Функция постобработки и фильтрации ограничивающих рамок и объектов.
# pred_bbox - массив необработанных ограничивающих рамок после предсказания.
# original_image - оригинальное изображение.
# input_size - размер входа сети.
# score_threshold - порог фильтрации объектов с низкой верояностью

def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):
    # Преобразуем тензор в массив numpy
    pred_bbox = np.array(pred_bbox)

    # Разделяем массив по значениям.
    # Координаты центра, длинна и ширина ограничивающей рамки.
    pred_xywh = pred_bbox[:, 0:4]
    # Показатель уверенности что в ограничивающей рамке есть объект.
    pred_conf = pred_bbox[:, 4]
    # Веротности присутствия в этой рамке каждого конкретно класса.
    pred_prob = pred_bbox[:, 5:]

    # Изменяем формат ограничивающих рамок.
    # (x, y, w, h) --> (xmin, ymin, xmax, ymax)
    # xmin = x - w/2, ymin = y - h/2
    # xmax = x + w/2, ymax = y + h/2
    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,
                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)
    
    # Сеть расчитывает размеры ограничивающих рамок 
    # отностительно размера своего входа = 416 х 416.
    # Нам нужно масштабировать размеры ограничивающих рамок
    # относительно изначального размера оригинального изображения.
    # (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org).
    # То есть производим изменения обратные функции image_preprocess.

    # Исходные размеры оригинального изображения.
    org_h, org_w = original_image.shape[:2]
    # Масштаб изменения размера.
    resize_ratio = min(input_size / org_w, input_size / org_h)

    # Вычисляем на сколько было сдвинуто изображение
    # по оси х (там, где заполнено серым).
    dw = (input_size - resize_ratio * org_w) / 2
    # На сколько сдвинуто по оси y.
    dh = (input_size - resize_ratio * org_h) / 2

    # Масштабируем значения координат оси х и убираем сдвиг.
    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio
    # Масштабируем значения координат оси y и убираем сдвиг.
    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio

    # Ограничиваем значения рамок, которые вышли  
    # за пределы границ изображения.
    # max([xmin, ymin], [0, 0])
    # min([xmax, ymax], [org_w - 1, org_h - 1])
    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),
                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)
    
    # Зададим маску неправильных ограничивающих рамок.
    # Условие: xmin > xmax или ymin > ymax
    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))
    # Присвоим 0 значениям, которые подпадают под условие маски.
    pred_coor[invalid_mask] = 0

    # Зададим допустимый диапазон масштабов рамок.
    valid_scale=[0, np.inf]

    # Вычислим масштаб каждой ограничивающей рамки.
    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))
    # Создадим маску по масштабу с условием, 
    # чтобы масштаб рамки был в допустимом диапазоне.
    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))

    # Находим класс объекта в каждой ограничивающей рамке.
    # 80 one hot encoding - > 1 index.
    classes = np.argmax(pred_prob, axis=-1)

    # Вычисляем итоговую вероятность объектов в ограничивающих рамках.
    # Умножаем показатель уверенности присутствия объекта в ограничивающей рамке
    # на вероятность класса, который имеет максимальную вероятность в этой рамке.
    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]
    
    # Создадим маску по вероятности с условием, 
    # чтобы верояность была выше допустимого порога.
    score_mask = scores > score_threshold

    # Создадим общую маску по масштабу и по вероятности.
    mask = np.logical_and(scale_mask, score_mask)

    # Отбрасываем недопустимые ограничивающие рамки по условию общей маски.
    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]

    # Соединяем всё в один массив и возвращаем.
    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)

# Функция для расчёта соотношения пересечения 
# двух ограничиващих рамок.
def bboxes_iou(boxes1, boxes2):
    # Преобразуем ограничивающие рамки в numpy массив
    # для удобной индексации.
    boxes1 = np.array(boxes1)
    boxes2 = np.array(boxes2)

    # Вычисляем площади каждой ограничивающей рамки.
    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])
    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])

    # Вычисляем какой ограничивающей рамки выбрать левый верхний угол.
    # max((xmin1, ymin1), (xmin2, ymin2))
    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])
    # Вычисляем какой ограничивающей рамки выбрать правый нижний угол.
    # min((xmax1, ymax1), (xmax2, ymax2))
    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])

    # Проверяем пересекаются ли рамки.
    # (xmax, ymax) - (xmin, ymin)
    inter_section = np.maximum(right_down - left_up, 0.0)

    # Расчитываем площадь пересечения.
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    # Вычисляем площадь объединения.
    union_area    = boxes1_area + boxes2_area - inter_area

    # Вычисляем Коэффициент Жаккара - соотношение пересечения по отношению к объединению.
    # Если полощадь пересечения будет очень маленькая, 
    # а площадь объединения очень большая, то при делении
    # мы можем получить крайне маленькое число.
    # Чтобы избежать проблем с вычислениями выбираем максимальное между
    # получившимся значением и машинный нулём для типа данных float32.
    ious = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)

    # Возвращаем расчитанный коэффициент Жаккара.
    return ious

# Сеть после предсказния выдаёт много разных рамок 
# одного и того же объекта с разной вероятностью.
# Чтобы выбрать наиболее подходящую и избавиться 
# от остальных используют алгоритм Non-maximum Suppression (NMS).

# Функция подавления не-максимумов позволяет отфильтровать
# менее вероятные ограничивающие рамки одних и тех же объектов.

# bboxes - рамки формата (xmin, ymin, xmax, ymax, score, class).
# iou_threshold - порог пересечения.

def nms(bboxes, iou_threshold):

    # Создадим список классов в данном изображении.
    classes_in_img = list(set(bboxes[:, 5]))

    # Список наилучших ограничивающих рамок с наибольшими вероятностями.
    best_bboxes = []

    # Перебираем все классы на изображении.
    for cls in classes_in_img:
        # Выбираем все ограничивающие рамки с текущим классом.
        cls_mask = (bboxes[:, 5] == cls)
        cls_bboxes = bboxes[cls_mask]

        # Пока список ограничивающих рамок текущего класса не станет пустым.
        while len(cls_bboxes) > 0:
            
            # Выбираем ограничивающую рамку с наивысшей вероятностью.
            max_ind = np.argmax(cls_bboxes[:, 4])
            best_bbox = cls_bboxes[max_ind]

            # Добавляем эту рамку в список наилучших.
            best_bboxes.append(best_bbox)

            # Создаём массив рамок текущего класса без наилучшей.
            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])
            
            # Рассчитываем соотношение пересечения наилучшей ограничивающей
            # рамки с отальными рамками этого класса.
            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])

            # Создаём массив весовых коэффициентов заполненный единицами.
            weight = np.ones((len(iou),), dtype=np.float32)

            # Зануляем весовой коэффициент тех рамок, у которых 
            # сооношение пересечения больше допустимого порога.
            iou_mask = iou > iou_threshold
            weight[iou_mask] = 0.0

            # Умножаем вероятность каждой рамки на её 
            # весовой коэффициент (0 или 1).
            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight
            
            # Оставляем те рамки, вероятность которых больше 0.
            score_mask = cls_bboxes[:, 4] > 0.
            cls_bboxes = cls_bboxes[score_mask]

    # Возвращаем список наилучших ограничивающих рамок.
    return best_bboxes

# Функция для рисования ограничивающих рамок
# и меток класса на изображении.

# image - изображение.
# bboxes - список ограничивающих рамок.
# show_label - нужно показывать метки или нет.
# show_confidence - отображение вероятности.
# text_color - цвет текста меток.
# rectangle_colors - цвета ограничивающих рамок.

def draw_bbox(image, bboxes, show_label=True, show_confidence = True, text_color='', rectangle_colors=''):   

    # Сохраняем размер изображения.
    image_h, image_w, image_ch = image.shape

    # Используем цветовое пространство hsv (Hue, Saturation, Value)
    # чтобы задать цвет класса объекта ограничивающей рамки.
    # Цвет задаётся Hue от 0 до 1, насыщенность и яркость берём максимально.
    # Присваиваем каждому классу свой цвет.
    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]

    # Преобразовывем цвета из пространства hsv в rgb.
    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))

    # Фиксируем зерно генератора случайных чисел,
    # чтобы каждый раз цвета перемешивались 
    # в одном и том же порядке.
    random.seed(0)
    # Перемешиваем цвета.
    random.shuffle(colors)
    # Снимаем фиксацию генератора случайных чисел.
    random.seed(None)

    # Перебираем ограничивающие рамки.
    for i, bbox in enumerate(bboxes):
        # Разделяем на координаты рамки,
        coor = np.array(bbox[:4], dtype=np.int32)
        # вероятность класса и
        score = bbox[4]
        # индекс класса.
        class_ind = int(bbox[5])

        # Выбираем использовать заданые извне цвета ограничивающих 
        # прямоугольников или со списка сгенерированных цветов.
        if rectangle_colors != '':
            bbox_color = rectangle_colors 
        else:
            bbox_color = colors[class_ind]

        # Выбираем использовать заданый извне цвет текста 
        # меток или со списка сгенерированных цветов.
        # Чтобы лучше было видно цвет текста сделаем
        # инвертированным относительно цвета рамки.
        if text_color != '':
            label_color = text_color
        else:
            r, g, b = colors[class_ind]
            label_color = (255 - r, 255 - g, 255 - b)

        # Расчитываем ширину линии прямоугольника 
        # исходя из размеров изображения.
        bbox_thick = int(0.6 * (image_h + image_w) / 1000)
        # Если ширина линии оказалась меньше 1, 
        # то повышаем значение до единицы.
        if bbox_thick < 1: 
            bbox_thick = 1

        # Расчитываем размер шрифта.
        fontScale = 0.75 * bbox_thick

        # Распаковываем значения координат.
        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])

        # Рисуем ограничивающий прямоугольник объекта на изображении.
        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)

        # Если установлен флаг отображения меток.
        if show_label:
            
            # Если нужно показывать вероятность.
            # Преобразовываем значение вероятности в строку.
            if show_confidence:
                score_str = " {:.2f}".format(score) 
            else:
                score_str = ""
            
            # Форимируем метку класса и вероятности.
            label = "{}".format(class_names_list[class_ind]) + score_str

            # Получаем размеры прямоугольника, который займёт текст.
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,
                                                                  fontScale, thickness=bbox_thick)
            # Рисуем заполненный прямоугольник для текста метки.
            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)

            # Пишем текст метки поверх прямоугольника.
            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        fontScale, label_color, bbox_thick, lineType=cv2.LINE_AA)

    # Возвращаем изображение с нарисованными ограничивающими рамками и вероятностями объектов.
    return image

#Функция обнаружения объектов на изображении с помощью Yolo.

# Yolo - модель сети.
# image_path - путь к изображению.
# input_size - размер входа сети (к этому размеру нужно привести входное изображение)
# score_threshold - порог вероятности предсказанного класса ! проверить
# iou_threshold - порог пересечения правильной ограничивающей рамки с предсказанной
# rectangle_colors - цвета ограничивающих рамок

def detect_image(Yolo, image_path, input_size=416, score_threshold=0.3, iou_threshold=0.48, text_color='', rectangle_colors=''):
    
    # Считываем изображение.
    original_image = cv2.imread(image_path)
    # По умолчанию opencv сохраняет изображение в формате BGR, 
    # а сеть принимает изображения в формате RGB.
    # Конвертируем изображение в формат RGB.
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

    # Производим предобработку входного изображения.
    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])
    # В полученное изображение добавляем одну ось для передачи в модель.
    image_data = image_data[np.newaxis, ...]

    # Передаём предобработанное изображение на вход сети,
    # на выходе получаем предсказанные сетью
    # объекты с ограничивающими рамками.
    pred_bbox = Yolo.predict(image_data)
    # После предсказания в переменной pred_bbox 
    # находится список с тремя массивами,
    # каждый соответствует выходу сети.
    # Вот форма этих массивов:
    # (1, 52, 52, 3, 85)
    # (1, 26, 26, 3, 85)
    # (1, 13, 13, 3, 85)
       
    # Далее мы объединяем все ограничивающие рамки каждого выхода.
    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]
    # Новая форма массивов в списке pred_bbox:
    # (8112, 85)
    # (2028, 85)
    # (507, 85)

    # Теперь мы объединяем ограничивающие рамки по всем выходам в один тензор.
    pred_bbox = tf.concat(pred_bbox, axis=0)
    # Форма тензора (10647, 85). 10647 = 8112 + 2028 + 507

    # Производим постобработку объектов с ограничивающими рамками.   
    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)
    # После постобработки выходной массив bboxes 
    # будет иметь форму (кол-во_рамок, 6).

    # Отфильтровываем лишние ограничивающие рамки с помощью подавления не-максимумов.
    bboxes = nms(bboxes, iou_threshold)

    # Рисуем ограничивающие рамки объектов на входном изображении.
    image = draw_bbox(original_image, bboxes, text_color=text_color, rectangle_colors=rectangle_colors)
    
    # Возвращаем изменённое изображение.
    return image



# Создаём модель сети Yolo.
yolo = Create_Yolo(input_size=yolo_input_size)
# Загружаем предобученные оригинальные веса в модель.
load_yolo_weights(yolo, 
                  yolo_weights_path)

!ls

/content/2138.jpg

demo_dir ='/content/'
images_path_list = [f for f in listdir(demo_dir) if isfile(join(demo_dir, f))]
images_path_list.sort()

# Подадим на вход сети каждое изображение и 
# отобразим ограничивающие рамки найденных обектов.
for image_path in images_path_list:
    # Путь к изображению.
    image_path = join(demo_dir, image_path)

    # Вариант отображения ограничивающих рамок:
    # Каждый класс имеет свой цвет ограничивающей рамки.
    image = detect_image(yolo, image_path, input_size=yolo_input_size)

    # Все объекты помечаются синей ограничивающей рамкой с белым текстом.
    # image = detect_image(yolo, image_path, input_size=yolo_input_size, text_color=(255, 255, 255), rectangle_colors=(255, 0, 0))
    
    # Отобразим выходное изображение.
    cv2_imshow(image)







PROJECT_NAME       = "stroi"
# Тип и версия модели yolo
YOLO_TYPE          = "yolov3"
# Имя модели.
MODEL_NAME         = f"{YOLO_TYPE}_{PROJECT_NAME}"
# Путь к оригинальным весам.
YOLO_V3_WEIGHTS    = "/content/yolov3.weights"
# Путь к файлу с списком имён классов COCO.
COCO_CLASSES       = "/content/Lesson_04.Train_yolov3/Model_data/coco.names"
# Шаги разделительной сетки в пикселях для каждого выхода сети.
STRIDES            = [8, 16, 32]
# Пороговое значение коэфициента пересечения, меньше 
# которого считается что в рамке нет объекта.
IOU_LOSS_THRESH    = 0.5
# Количество якорей на один выход сети.
ANCHOR_PER_SCALE   = 3
# Максимальное количество ограничивающих рамок на выход сети.
MAX_BBOX_PER_SCALE = 100
# Размер входа сети (размер входных изображений).
INPUT_IMG_SIZE     = 416

# Якоря для каждого выхода сети.
YOLO_ANCHORS       = [[[10,  13], [16,   30], [33,   23]],
                      [[30,  61], [62,   45], [59,  119]],
                      [[116, 90], [156, 198], [373, 326]]]

# Флаг сохранения только лучшей модели по значению 
# функции потерь на проверочном наборе. Рекомендуется True. 
# Программа будет сравнивать val_loss и сохранять только лучшую.
SAVE_BEST_ONLY     = True
# Флаг сохранения модели после каждой эпохи. Понадобиться много 
# дискового пространства. Рекомендуется False.
SAVE_CHECKPOINT    = False
# Путь к контрольным точкам обучения.
CHECKPOINTS_FOLDER = "/content/Lesson_04.Train_yolov3/Checkpoints"

# Путь к файл со списком имён классов для обучения.
CLASSES_PATH       = "/content/Lesson_04.Train_yolov3/Datasets/fire_dataset/fire_names.txt"
# Путь к файл с аннотациями датасета обучения.
TRAIN_ANNOT_PATH   = "/content/Lesson_04.Train_yolov3/Datasets/fire_dataset/fire_train.txt"
# Путь, где будут сохранятся логи обучения и проверки.
LOGDIR             = "Logs"

# Загрузка изображений в оперативную память. 
# Ускорит обучение, но потребуется много памяти.
LOAD_IMAGES_TO_RAM = True
# Размер пакета для обучения.
TRAIN_BATCH_SIZE   = 4
# Аугментация данных для обучения
TRAIN_DATA_AUG     = True
# Использование предобученых оригинальных весов.
TRAIN_TRANSFER     = True
# Обучение с контрольной точки. Флаг либо путь к чекпоинту.
FROM_CHECKPOINT    = False # или "Checkpoints/fire_yolov3"
# Скорость обучения.
LEARNING_RATE      = 1e-4
# Количество эпох.
EPOCHS             = 5

# Путь к файл с аннотациями для проверочного набора.
TEST_ANNOT_PATH    = "/content/Lesson_04.Train_yolov3/Datasets/fire_dataset/fire_test.txt"
# Размер проверочного пакета.
TEST_BATCH_SIZE    = 4
# Аугментации проверочных данных.
TEST_DATA_AUG      = False

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization, MaxPool2D
from tensorflow.keras.regularizers import l2

import cv2
import time
import random
import colorsys




STRIDES         = np.array(STRIDES)
ANCHORS         = (np.array(YOLO_ANCHORS).T/STRIDES).T

# Функция для загрузки имён классов из файла.
def read_class_names(class_file_name):
    # Создаём пустой словарь для хранения 
    # индекса и имени класса обекта.
    names = {}
    # Открываем файл с именами классов в режиме чтения.
    with open(class_file_name, 'r') as data:
        # Проходим по строкам файла. В файле имена 
        # классов написаны каждый с новой строки.
        for ID, name in enumerate(data):
            # Сохраняем в словарь имя класса 
            # по его индексу.
            names[ID] = name.strip('\n')
    # Возвращаем словарь индексов и имён классов.
    return names

# Переопределим класс пакетной нормализации.
class BatchNormalization(BatchNormalization):
    # В TensorFlow 2.0 есть особенности заморозки слоя BatchNormalization.
    # Пакетная нормализация работает по-разному во время обучения 
    # и во время вывода (predict, evaluate).
    # Чтобы не изменять внутренние обученные переменные 
    # добавим возможность заморозки значений слоя.
    def call(self, x, training=False):
        if not training:
            training = tf.constant(False)
        training = tf.logical_and(training, self.trainable)
        return super().call(x, training)

# Зададим функцию для увеличение разрешения.
def upsample(input_layer):
    # Нам нужно будет увеличивать выход слоя сети, 
    # растягивать в 2 раза по ширине и высоте.
    # Для этого будем использовать интерполяцию методом ближайшего соседа.
    # Это самый простой и быстрый метод.
    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')
    
# Зададим функцию для определения парамметров свёрточного слоя.
def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):
    # Если уменьшение разрешения активно
    if downsample:
        # заполняем нулями сверху и слева
        # ((top_pad, bottom_pad), (left_pad, right_pad))
        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)
        padding = 'valid' # без заполнения
        strides = 2 # шаг свёртки
    else:
        strides = 1 # шаг свёртки
        padding = 'same' # заполнение до такого же размера

    # Определяем параметры свёртки.
    # filters - количество фильтров
    # kernel_size - размер ядра свёртки
    # strides - шаг ядра свёртки
    # padding - заполнение
    # use_bias - использование смещения
    # kernel_regularizer - регуляризация весов
    # kernel_initializer - инициализация весов
    # bias_initializer - инициализация весов смещения
    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,
                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),
                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),
                  bias_initializer=tf.constant_initializer(0.))(input_layer)
    
    # Если активна пакетная нормализация.
    if bn:
        conv = BatchNormalization()(conv)

    # Если нужно использовать функцию активации.
    if activate == True:
        # используется ReLU с "утечкой"
        conv = LeakyReLU(alpha=0.1)(conv)

    # Возвращаем определённый свёрточный слой.
    return conv
    
# Зададим функцию для определения блока короткого соединения.
def residual_block(input_layer, input_channel, filter_num1, filter_num2):
    # Сохраняем значения короткого соединения на входе в блок
    short_cut = input_layer

    # Пропускаем через слои свёртки.
    # Cвёртка 1 х 1 для уменьшения размерности 
    # (свёртка 1 х 1 объединяет все каналы входных пикселей в один пиксель).
    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))
    # свёртка 3 х 3 
    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))

    # Отстаточные или короткие соединения нужны для того,
    # чтобы избавится от проблемы затухания градиента
    # в глубоких сетях с большим количеством слоёв

    # Складываем значения короткого соединения с выходными значениями свёртки
    residual_output = short_cut + conv

    # Возвращаем полученный результат
    return residual_output
    
# Зададим архитектуру сети Darknet-53.
def darknet53(input_data):
    # свёртка
    input_data = convolutional(input_data, (3, 3,  3,  32))
    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)

    # Остаточный блок - это блок свёртки с
    # коротким прямым соединением входа с выходом.
    for i in range(1):
        input_data = residual_block(input_data,  64,  32, 64)

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)

    # остаточные блоки
    for i in range(2):
        input_data = residual_block(input_data, 128,  64, 128)

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)

    # остаточные блоки
    for i in range(8):
        input_data = residual_block(input_data, 256, 128, 256)

    # сохраняем ответвеление маршрута
    route_1 = input_data

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)

    # остаточные блоки
    for i in range(8):
        input_data = residual_block(input_data, 512, 256, 512)

    # сохраняем ответвеление маршрута
    route_2 = input_data

    # свёртка с уменьшением
    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)

    # остаточные блоки
    for i in range(4):
        input_data = residual_block(input_data, 1024, 512, 1024)

    # возращаем ответвления и выход сети 
    return route_1, route_2, input_data
    
# Зададим архитектуру сети YOLOv3.
def YOLOv3(input_layer, NUM_CLASS):
    # В качестве входного блока используется сеть Darknet-53.
    # На выходе сети имеем два ответвления и выход сети
    route_1, route_2, conv = darknet53(input_layer)

    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3,  512, 1024))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3,  512, 1024))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 1024,  512))
    # свёртка 3 х 3
    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))
    
    # Первый выход сети для предсказания объектов
    # большого размера, имеет форму [None, 13, 13, 255]. 
    # 255 получается если кол-во классов (80) + 5 значений:
    # это координаты центра объекта x и y,
    # ширина и высота ограничивающей рамки w и h,
    # и показатель уверенности что в ограничивающей рамке есть объект.
    # 3 - это количество якорей ограничивающих рамок.
    # 13 x 13 - это размер сетки для больших объектов,
    # которой делится входное изображение.
    # Большие объекты -> меньше делений сетки.
    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS+5)), activate=False, bn=False)

    # свёртка 1 х 1 с основным маршрутом сети
    conv = convolutional(conv, (1, 1,  512,  256))

    # Растягиваем текущий выход сети для совпадения размеров
    # со следующим слоем
    conv = upsample(conv)

    # Присоединяем текущий маршрут с ответвлением сети Darknet-53
    # вдоль последней оси
    conv = tf.concat([conv, route_2], axis=-1)
    
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 768, 256))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 256, 512))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 512, 256))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 256, 512))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 512, 256))
    # свёртка 3 х 3
    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))

    # Второй выход сети для предсказания объектов
    # среднего размера, имеет форму [None, 26, 26, 255] 
    # 26 x 26 - это размер сетки для средних объектов.
    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS+5)), activate=False, bn=False)

    # свёртка 1 х 1 с основным маршрутом сети
    conv = convolutional(conv, (1, 1, 256, 128))

    # Растягиваем текущий выход сети для совпадения размеров
    # со следующим слоем
    conv = upsample(conv)

    # Присоединяем текущий маршрут с ответвлением сети Darknet-53
    # вдоль последней оси
    conv = tf.concat([conv, route_1], axis=-1)

    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 384, 128))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 128, 256))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 256, 128))
    # свёртка 3 х 3
    conv = convolutional(conv, (3, 3, 128, 256))
    # свёртка 1 х 1
    conv = convolutional(conv, (1, 1, 256, 128))
    # свёртка 3 х 3
    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))
    
    # Третий выход сети для предсказания объектов
    # маленького размера, имеет форму [None, 52, 52, 255] 
    # 52 x 52 - это размер сетки для маленьких объектов,
    # которой делится входное изображение.
    # маленькие объекты -> больше делений сетки.
    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS+5)), activate=False, bn=False)
        
    # Возвращаем список из трёх выходов сети
    return [conv_sbbox, conv_mbbox, conv_lbbox]
    
# Зададим функцию преобразования значений каждого выхода сети к последовательности:
# координаты обекта, ширина и высота ограничивающей рамки, 
# показатель уверенности присутствия объекта и вероятности классов.
def decode(conv_output, NUM_CLASS, i=0):
    # где i = 0, 1 или 2, чтобы соответствовать трем масштабам сетки (выходам сети)
    # форма выхода сети
    conv_shape = tf.shape(conv_output)
    # размер пакета
    batch_size = conv_shape[0]
    # размер выхода сети (13, 26, 52)
    output_size = conv_shape[1]

    # изменяем форму выхода сети, чтобы разделить данные 
    # по трём видам используемых якорей ограничивающих рамок
    # 255 -> (80 + 5) * 3 (как объяснялось выше)
    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))

    # Разделяем данные, каждый параметр в свой массив.
    # Смещение центрального положения объекта.
    # Необработанные значения x и y
    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]
    # Предсказанные длинна и ширина ограничивающей рамки
    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]
    # Показатель уверенности что в ограничивающей рамке есть объект
    conv_raw_conf = conv_output[:, :, :, :, 4:5]
    # Веротности присутствия в этой рамке каждого конкретно класса
    conv_raw_prob = conv_output[:, :, :, :, 5: ]

    # Создаём сетку, где output_size равен выходу сети 13, 26, 52.
    # Здесь в комментариях для примера output_size = 13.
    # Создаём вектор в диапазоне от 0 до output_size-1.
    y = tf.range(output_size, dtype=tf.int32) # [0, 1, ... 12]
    # конвертируем в вектор-столбец
    y = tf.expand_dims(y, -1) # [[0], [1], ... [12]]
    # Повторяем вектор output_size раз
    # и получаем матрицу 13 х 13
    # со значениями от 0 до 12 сверху вниз
    y = tf.tile(y, [1, output_size])
    
    # Создаём вектор в диапазоне от 0 до output_size-1
    x = tf.range(output_size, dtype=tf.int32 )# [0, 1, ... 12]
    # конвертируем в вектор-строку
    x = tf.expand_dims(x, 0) # [[0, 1, ... 12]]
    # Повторяем вектор output_size раз
    # и получаем матрицу 13 х 13
    # со значениями от 0 до 12 слева направо
    x = tf.tile(x, [output_size, 1])

    # Cоздаём в каждой матрице 3 ось
    # и соеденяем 2 матрицы по 3тей оси.
    # Получаем матрицу формы (13, 13, 2),
    # в которой координаты каждого фрагмента сетки.
    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)

    # Преобразовываем сетку к форме (batch_size, 13, 13, 3, 2)
    # чтобы форма совпадала с формой якорей.
    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])
    # Приводим к типу float32.
    xy_grid = tf.cast(xy_grid, tf.float32)

    # Используя формулы с рисунка
    # расчитываем координаты центра объекта.
    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]
    
    # Вычисляем длину и ширину ограничивающей рамки используя якоря.
    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]

    # Соединяем вместе значения координат объекта 
    # и ширины, высоты ограничивающей рамки.
    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)

    # Расчитываем уверенность.
    pred_conf = tf.sigmoid(conv_raw_conf)
    # Вычисляем вероятности классов.
    pred_prob = tf.sigmoid(conv_raw_prob)

    # Соединяем все значения вместе и возвращаем.
    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)
    
# Функция для создания модели Yolo
def Create_Yolo(input_size=416, channels=3, training=False, CLASSES=COCO_CLASSES):
    NUM_CLASS = len(read_class_names(CLASSES))
    # Создаём вход модели с формой 416 x 416 x 3.
    input_layer  = Input([input_size, input_size, channels])

    # Добавляем сеть YOLOv3.
    conv_tensors = YOLOv3(input_layer, NUM_CLASS)

    # Создаём список выходных тензоров.
    output_tensors = []

    # Перебираем каждый выход сети,
    for i, conv_tensor in enumerate(conv_tensors):
        # и декодируем его.
        pred_tensor = decode(conv_tensor, NUM_CLASS, i)
        # Если будем обучать
        if training: 
            # добавляем недекодированные выходные тензоры сети
            # для расчёта функци потерь.
            output_tensors.append(conv_tensor)
            
        # Добавляем декодированный выход сети.
        output_tensors.append(pred_tensor)

    # Создаём keras модель и возвращаем её.
    Yolo = tf.keras.Model(input_layer, output_tensors)
    return Yolo
    
    
# Функция для загрузки весов в модель.
def load_yolo_weights(model, weights_file):
    # Очищаем сессию чтобы сбросить имена слоёв.
    tf.keras.backend.clear_session()
    # Загружаем оригинальные веса в модель.
    # Yolo имеет 75 свёрточных слоёв.
    range1 = 75 
    # Номера выходных слоёв.
    range2 = [58, 66, 74]   

    # Открываем файл с весами модели.
    with open(weights_file, 'rb') as wf:
        # Считываем данные с файла.
        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)

        # Счётчик для слоёв пакетной нормализации.
        j = 0
        # Пербираем слои.
        for i in range(range1):
            
            # Присваиваем имя свёрточному слою и номер по порядку.
            if i > 0:
                conv_layer_name = 'conv2d_%d' %i
            else:
                conv_layer_name = 'conv2d'
                
            # Присваиваем имя слою пакетной нормализации и номер по порядку.
            if j > 0:
                bn_layer_name = 'batch_normalization_%d' %j
            else:
                bn_layer_name = 'batch_normalization'
            
            # Получаем свёрточный слой по имени из модели.
            conv_layer = model.get_layer(conv_layer_name)
            # Из слоя получаем параметры:
            # количество фильтров
            filters = conv_layer.filters
            # размер ядра свёртки
            k_size = conv_layer.kernel_size[0]
            # размер входа слоя
            in_dim = conv_layer.input_shape[-1]

            # Если текущий слой не выходной (используется пакетная нормализация).
            if i not in range2:
                # Считываем с файла парамметры слоя пакетной нормализации.
                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)
                # В фреймворке darknet используется следующий порядок параметров:
                # [beta, gamma, mean, variance].

                # Мы же используем tensorflow, в котором другой
                # порядок следования: [gamma, beta, mean, variance].
                # Изменяем форму и порядок следования.
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]
                
                # Получаем слой пакетной нормализации по имени из модели.
                bn_layer = model.get_layer(bn_layer_name)
                # Увеличиваем счётчик слоёв пакетной нормализации.
                j += 1 
            else:
                # Если слой без батч нотмализации, значит в слое
                # используются нейроны смещения.
                # Считываем значения весов для смещения.
                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)

            # В фреймворке Darknet используется форма
            # свёрточного слоя: (out_dim, in_dim, height, width).
            # В tensorflow: (height, width, in_dim, out_dim)

            # Объявляем кортеж с параметрами формы свёрточного слоя.
            conv_shape = (filters, in_dim, k_size, k_size)

            # Считываем с файла все значения весов слоя.
            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))
            # Изменяем форму и порядок следования.
            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])

            if i not in range2:
                # Если слой не выходной,
                # устанавливаем значения весов в слой модели.
                # Устанавливаем веса свёрточного слоя.
                conv_layer.set_weights([conv_weights])
                # Устанавливаем веса слоя пакетной нормализации.
                bn_layer.set_weights(bn_weights)
            else:
                # Если слой выходной,
                # устанавливаем значения весов свёртки и смещения.
                conv_layer.set_weights([conv_weights, conv_bias])

        # Проверяем пустой ли файл с весами.
        assert len(wf.read()) == 0, 'Failed to read all data'
        
# Функция предобработки входного изображения.
# image - входное изображение.
# target_size - необходимый размер.

# Сеть на вход ожидает квадратное изображение
# размером 416 х 416 х 3.
# Однако размеры и форма входных изображений могут быть другими.
# Для этого изменим размер изображения к необходимому
# не изменяя пропорции. Если изображение прямогуольное,
# то расположим его по центру, а края заполним серым цветом.

def image_preprocess(image, target_size, gt_boxes=None):
    # image - исходное изображение
    # target_size - размер, к которому нужно привести изображение
    # gt_boxes - ограничивающие рамки, которые тоже нужно 
    # преобразовать относительно изменённого изображения.
    
    # Размеры, к которым нужно привести изображение.
    ih, iw = target_size
    # Текущие размеры изображения.
    h,  w, _ = image.shape

    # Расчитываем масштаб изменения размера.
    scale = min(iw/w, ih/h)
    # Умножаем значение ширины и высоты изображения на масштаб.
    nw, nh  = int(scale * w), int(scale * h)
    # Изменяем размер изображения к необходимому.
    image_resized = cv2.resize(image, (nw, nh))

    # Создаём пустое изображение, заполненое серым цветом.
    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)

    # Расчитываем координаты куда копировать масштабированное
    # изображение, чтобы оно было по центру.
    dw, dh = (iw - nw) // 2, (ih-nh) // 2
    
    # Копируем масштабированное изображение в пустое серое.
    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized
                                    
    # Полученное изображение переводим в тип данных float32.
    image_paded = image_paded.astype(np.float32)
    
    # Приводим значения интенсивности пикселей 
    # к диапазону от 0 до 1.
    image_paded = image_paded / 255.0
   
    # Если ограничивающие рамки определены, то
    # их тоже нужно преобразовать относительно
    # необходимого размера.
    if gt_boxes is None:
        # Возвращаем только преобразованное изображение.                                            
        return image_paded
    else:
        # Масштабируем и сдвигаем рамки.
        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw
        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh
        
        # Возвращаем преобразованное изображение и рамки.
        return image_paded, gt_boxes
    
# Функция постобработки и фильтрации ограничивающих рамок и объектов.
# pred_bbox - массив необработанных ограничивающих рамок после предсказания.
# original_image - оригинальное изображение.
# input_size - размер входа сети.
# score_threshold - порог фильтрации объектов с низкой верояностью

def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):
    # Преобразуем тензор в массив numpy
    pred_bbox = np.array(pred_bbox)

    # Разделяем массив по значениям.
    # Координаты центра, длинна и ширина ограничивающей рамки.
    pred_xywh = pred_bbox[:, 0:4]
    # Показатель уверенности что в ограничивающей рамке есть объект.
    pred_conf = pred_bbox[:, 4]
    # Веротности присутствия в этой рамке каждого конкретно класса.
    pred_prob = pred_bbox[:, 5:]

    # Изменяем формат ограничивающих рамок.
    # (x, y, w, h) --> (xmin, ymin, xmax, ymax)
    # xmin = x - w/2, ymin = y - h/2
    # xmax = x + w/2, ymax = y + h/2
    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,
                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)
    
    # Сеть расчитывает размеры ограничивающих рамок 
    # отностительно размера своего входа = 416 х 416.
    # Нам нужно масштабировать размеры ограничивающих рамок
    # относительно изначального размера оригинального изображения.
    # (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org).
    # То есть производим изменения обратные функции image_preprocess.

    # Исходные размеры оригинального изображения.
    org_h, org_w = original_image.shape[:2]
    # Масштаб изменения размера.
    resize_ratio = min(input_size / org_w, input_size / org_h)

    # Вычисляем на сколько было сдвинуто изображение
    # по оси х (там, где заполнено серым).
    dw = (input_size - resize_ratio * org_w) / 2
    # На сколько сдвинуто по оси y.
    dh = (input_size - resize_ratio * org_h) / 2

    # Масштабируем значения координат оси х и убираем сдвиг.
    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio
    # Масштабируем значения координат оси y и убираем сдвиг.
    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio

    # Ограничиваем значения рамок, которые вышли  
    # за пределы границ изображения.
    # max([xmin, ymin], [0, 0])
    # min([xmax, ymax], [org_w - 1, org_h - 1])
    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),
                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)
    
    # Зададим маску неправильных ограничивающих рамок.
    # Условие: xmin > xmax или ymin > ymax
    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))
    # Присвоим 0 значениям, которые подпадают под условие маски.
    pred_coor[invalid_mask] = 0

    # Зададим допустимый диапазон масштабов рамок.
    valid_scale=[0, np.inf]

    # Вычислим масштаб каждой ограничивающей рамки.
    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))
    # Создадим маску по масштабу с условием, 
    # чтобы масштаб рамки был в допустимом диапазоне.
    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))

    # Находим класс объекта в каждой ограничивающей рамке.
    # 80 one hot encoding - > 1 index.
    classes = np.argmax(pred_prob, axis=-1)

    # Вычисляем итоговую вероятность объектов в ограничивающих рамках.
    # Умножаем показатель уверенности присутствия объекта в ограничивающей рамке
    # на вероятность класса, который имеет максимальную вероятность в этой рамке.
    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]
    
    # Создадим маску по вероятности с условием, 
    # чтобы верояность была выше допустимого порога.
    score_mask = scores > score_threshold

    # Создадим общую маску по масштабу и по вероятности.
    mask = np.logical_and(scale_mask, score_mask)

    # Отбрасываем недопустимые ограничивающие рамки по условию общей маски.
    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]

    # Соединяем всё в один массив и возвращаем.
    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)
    
# Функция для расчёта соотношения пересечения двух 
# ограничиващих рамок в формате xmin ymin xmax ymax.
def bbox_xyxy_iou(boxes1, boxes2):
    # Преобразуем ограничивающие рамки в numpy массив
    # для удобной индексации.
    boxes1 = np.array(boxes1)
    boxes2 = np.array(boxes2)

    # Вычисляем площади каждой ограничивающей рамки.
    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])
    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])

    # Вычисляем какой ограничивающей рамки выбрать левый верхний угол.
    # max((xmin1, ymin1), (xmin2, ymin2))
    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])
    # Вычисляем какой ограничивающей рамки выбрать правый нижний угол.
    # min((xmax1, ymax1), (xmax2, ymax2))
    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])

    # Проверяем пересекаются ли рамки.
    # (xmax, ymax) - (xmin, ymin)
    inter_section = np.maximum(right_down - left_up, 0.0)

    # Расчитываем площадь пересечения.
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    # Вычисляем площадь объединения.
    union_area    = boxes1_area + boxes2_area - inter_area

    # Вычисляем коэффициент Жаккара - соотношение пересечения по отношению к объединению.
    # Если полощадь пересечения будет очень маленькая, 
    # а площадь объединения очень большая, то при делении
    # мы можем получить крайне маленькое число.
    # Чтобы избежать проблем с вычислениями выбираем максимальное между
    # получившимся значением и машинный нулём для типа данных float32.
    ious = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)

    # Возвращаем расчитанный коэффициент Жаккара.
    return ious

# Функция для расчёта соотношения пересечения двух 
# ограничиващих рамок в формате x y h w.
def bbox_xywh_iou(boxes1, boxes2):
    # Вычисляем площади каждой ограничивающей рамки.
    boxes1_area = boxes1[..., 2] * boxes1[..., 3]
    boxes2_area = boxes2[..., 2] * boxes2[..., 3]
    
    # Переводим формат ограничивающих рамок.
    # x y w h -> xmin ymin xmax ymax
    # xmin = x - w/2, ymin = y - h/2
    # xmax = x + w/2, ymax = y + h/2
    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,
                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)
    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,
                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)

    # Вычисляем какой рамки выбрать левый верхний угол.
    # max((xmin1, ymin1), (xmin2, ymin2))
    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])
    # Вычисляем какой рамки выбрать правый нижний угол.
    # min((xmax1, ymax1), (xmax2, ymax2))
    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])

    # Проверяем пересекаются ли рамки.
    # (xmax, ymax) - (xmin, ymin)
    inter_section = tf.maximum(right_down - left_up, 0.0)
    
    # Расчитываем площадь пересечения.
    inter_area = inter_section[..., 0] * inter_section[..., 1]
    
    # Вычисляем площадь объединения.
    union_area = boxes1_area + boxes2_area - inter_area
    
    # Вычисляем отношение площади пересечения к площади 
    # объединения (коэффициент Жаккара) и возвращаем его.
    return 1.0 * inter_area / union_area
    
# Сеть после предсказния выдаёт много разных рамок 
# одного и того же объекта с разной вероятностью.
# Чтобы выбрать наиболее подходящую и избавиться 
# от остальных используют алгоритм Non-maximum Suppression (NMS).

# Функция подавления не-максимумов позволяет отфильтровать
# менее вероятные ограничивающие рамки одних и тех же объектов.

# bboxes - рамки формата (xmin, ymin, xmax, ymax, score, class).
# iou_threshold - порог пересечения.

def nms(bboxes, iou_threshold):

    # Создадим список классов в данном изображении.
    classes_in_img = list(set(bboxes[:, 5]))

    # Список наилучших ограничивающих рамок с наибольшими вероятностями.
    best_bboxes = []

    # Перебираем все классы на изображении.
    for cls in classes_in_img:
        # Выбираем все ограничивающие рамки с текущим классом.
        cls_mask = (bboxes[:, 5] == cls)
        cls_bboxes = bboxes[cls_mask]

        # Пока список ограничивающих рамок текущего класса не станет пустым.
        while len(cls_bboxes) > 0:
            
            # Выбираем ограничивающую рамку с наивысшей вероятностью.
            max_ind = np.argmax(cls_bboxes[:, 4])
            best_bbox = cls_bboxes[max_ind]

            # Добавляем эту рамку в список наилучших.
            best_bboxes.append(best_bbox)

            # Создаём массив рамок текущего класса без наилучшей.
            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])
            
            # Рассчитываем соотношение пересечения наилучшей ограничивающей
            # рамки с отальными рамками этого класса.
            iou = bbox_xyxy_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])

            # Создаём массив весовых коэффициентов заполненный единицами.
            weight = np.ones((len(iou),), dtype=np.float32)

            # Зануляем весовой коэффициент тех рамок, у которых 
            # сооношение пересечения больше допустимого порога.
            iou_mask = iou > iou_threshold
            weight[iou_mask] = 0.0

            # Умножаем вероятность каждой рамки на её 
            # весовой коэффициент (0 или 1).
            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight
            
            # Оставляем те рамки, вероятность которых больше 0.
            score_mask = cls_bboxes[:, 4] > 0.
            cls_bboxes = cls_bboxes[score_mask]

    # Возвращаем список наилучших ограничивающих рамок.
    return best_bboxes
    
# Функция для рисования ограничивающих рамок
# и меток класса на изображении.

# image - изображение.
# bboxes - список ограничивающих рамок.
# show_label - нужно показывать метки или нет.
# show_confidence - отображение вероятности.
# text_color - цвет текста меток.
# rectangle_colors - цвета ограничивающих рамок.

def draw_bbox(image, bboxes, CLASSES=COCO_CLASSES, show_label=True, show_confidence = True, text_color='', rectangle_colors=''):   
    # Считываем имена классов.
    NUM_CLASS = read_class_names(CLASSES)
    # Количество классов.
    num_classes = len(NUM_CLASS)
    # Сохраняем размер изображения.
    image_h, image_w, image_ch = image.shape

    # Используем цветовое пространство hsv (Hue, Saturation, Value)
    # чтобы задать цвет класса объекта ограничивающей рамки.
    # Цвет задаётся Hue от 0 до 1, насыщенность и яркость берём максимально.
    # Присваиваем каждому классу свой цвет.
    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]

    # Преобразовывем цвета из пространства hsv в rgb.
    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))

    # Фиксируем зерно генератора случайных чисел,
    # чтобы каждый раз цвета перемешивались 
    # в одном и том же порядке.
    random.seed(0)
    # Перемешиваем цвета.
    random.shuffle(colors)
    # Снимаем фиксацию генератора случайных чисел.
    random.seed(None)

    # Перебираем ограничивающие рамки.
    for i, bbox in enumerate(bboxes):
        # Разделяем на координаты рамки,
        coor = np.array(bbox[:4], dtype=np.int32)
        # вероятность класса и
        score = bbox[4]
        # индекс класса.
        class_ind = int(bbox[5])

        # Выбираем использовать заданые извне цвета ограничивающих 
        # прямоугольников или со списка сгенерированных цветов.
        if rectangle_colors != '':
            bbox_color = rectangle_colors 
        else:
            bbox_color = colors[class_ind]

        # Выбираем использовать заданый извне цвет текста 
        # меток или со списка сгенерированных цветов.
        # Чтобы лучше было видно цвет текста сделаем
        # инвертированным относительно цвета рамки.
        if text_color != '':
            label_color = text_color
        else:
            r, g, b = colors[class_ind]
            label_color = (255 - r, 255 - g, 255 - b)

        # Расчитываем ширину линии прямоугольника 
        # исходя из размеров изображения.
        bbox_thick = int(0.6 * (image_h + image_w) / 1000)
        # Если ширина линии оказалась меньше 1, 
        # то повышаем значение до единицы.
        if bbox_thick < 1: 
            bbox_thick = 1

        # Расчитываем размер шрифта.
        fontScale = 0.75 * bbox_thick

        # Распаковываем значения координат.
        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])

        # Рисуем ограничивающий прямоугольник объекта на изображении.
        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)

        # Если установлен флаг отображения меток.
        if show_label:
            
            # Если нужно показывать вероятность.
            # Преобразовываем значение вероятности в строку.
            if show_confidence:
                score_str = " {:.2f}".format(score) 
            else:
                score_str = ""
            
            # Форимируем метку класса и вероятности.
            label = "{}".format(NUM_CLASS[class_ind]) + score_str

            # Получаем размеры прямоугольника, который займёт текст.
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,
                                                                  fontScale, thickness=bbox_thick)
            # Рисуем заполненный прямоугольник для текста метки.
            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)

            # Пишем текст метки поверх прямоугольника.
            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,
                        fontScale, label_color, bbox_thick, lineType=cv2.LINE_AA)

    # Возвращаем изображение с нарисованными ограничивающими рамками и вероятностями объектов.
    return image
    
# Функция обнаружения объектов на изображении с помощью Yolo.

# Yolo - модель сети.
# image_path - путь к изображению.
# input_size - размер входа сети (к этому размеру нужно привести входное изображение)
# score_threshold - порог вероятности предсказанного класса ! проверить
# iou_threshold - порог пересечения правильной ограничивающей рамки с предсказанной
# rectangle_colors - цвета ограничивающих рамок

def detect_image(Yolo, image_path, input_size=416, CLASSES=COCO_CLASSES, score_threshold=0.3, iou_threshold=0.45, text_color='', rectangle_colors=''):
    
    # Считываем изображение.
    original_image = cv2.imread(image_path)
    # По умолчанию opencv сохраняет изображение в формате BGR, 
    # а сеть принимает изображения в формате RGB.
    # Конвертируем изображение в формат RGB.
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

    # Производим предобработку входного изображения.
    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])
    # В полученное изображение добавляем одну ось для передачи в модель.
    image_data = image_data[np.newaxis, ...]

    # Передаём предобработанное изображение на вход сети,
    # на выходе получаем предсказанные сетью
    # объекты с ограничивающими рамками.
    pred_bbox = Yolo.predict(image_data)
    # После предсказания в переменной pred_bbox 
    # находится список с тремя массивами,
    # каждый соответствует выходу сети.
    # Вот форма этих массивов:
    # (1, 52, 52, 3, 85)
    # (1, 26, 26, 3, 85)
    # (1, 13, 13, 3, 85)
       
    # Далее мы объединяем все ограничивающие рамки каждого выхода.
    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]
    # Новая форма массивов в списке pred_bbox:
    # (8112, 85)
    # (2028, 85)
    # (507, 85)

    # Теперь мы объединяем ограничивающие рамки по всем выходам в один тензор.
    pred_bbox = tf.concat(pred_bbox, axis=0)
    # Форма тензора (10647, 85). 10647 = 8112 + 2028 + 507

    # Производим постобработку объектов с ограничивающими рамками.   
    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)
    # После постобработки выходной массив bboxes 
    # будет иметь форму (кол-во_рамок, 6).

    # Отфильтровываем лишние ограничивающие рамки с помощью подавления не-максимумов.
    bboxes = nms(bboxes, iou_threshold)

    # Рисуем ограничивающие рамки объектов на входном изображении.
    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, text_color=text_color, rectangle_colors=rectangle_colors)
    
    # Возвращаем изменённое изображение.
    return image

import os
import cv2
import random
import numpy as np
import tensorflow as tf




# Создадим класс набора данных.
class Dataset(object):
    # Объявим переменные внутри класса.
    def __init__(self, dataset_type):
        # Путь к текстовому документу с аннотациями.
        self.annot_path  = TRAIN_ANNOT_PATH if dataset_type == '/content/Lesson_04.Train_yolov3/Datasets/fire_dataset/train' else TEST_ANNOT_PATH
        # Размер изображения входа сети.
        self.input_size = INPUT_IMG_SIZE
        # Размер пакета.
        self.batch_size = TRAIN_BATCH_SIZE if dataset_type == 'train' else TEST_BATCH_SIZE
        # Аугментация данных.
        self.data_aug = TRAIN_DATA_AUG if dataset_type == 'train' else TEST_DATA_AUG
        # Список шагов выходов сети (размер в пикселях
        # ячейки разделительной сетки изображения).
        self.strides = np.array(STRIDES)
        # Список имён классов.
        self.classes = read_class_names(CLASSES_PATH)
        # Количество класссов.
        self.num_classes = len(self.classes)
        # Список якорей.
        self.anchors = (np.array(YOLO_ANCHORS).T/self.strides).T
        # Количество якорей на выход сети.
        self.anchor_per_scale = ANCHOR_PER_SCALE
        # Максимальное количество ограничивающих рамок на выход сети.
        self.max_bbox_per_scale = MAX_BBOX_PER_SCALE
        # Загружаем аннотации.
        self.annotations = self.load_annotations(dataset_type)
        # Количество образцов в датасете.
        self.num_samples = len(self.annotations)
        # Количество пакетов данных.
        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))
        # Счётчик пакетов.
        self.batch_count = 0
        
    # Функция загрузки аннотаций.
    def load_annotations(self, dataset_type):
        # dataset_type - тип датасета: 'train' или 'test'
        
        # Список для окончательных аннотаций.
        final_annotations = []
        # Считываем текст с файла аннотаций.
        with open(self.annot_path, 'r') as f:
            # Считываем строки.
            txt = f.readlines()
            # Используем строку аннотации, если она имеет метки.
            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]
        
        # Перемешиваем аннотации.
        np.random.shuffle(annotations)
        
        # Перебираем строки аннотаций.
        for annotation in annotations:
            # Разделяем строку по пробелам.
            line = annotation.split()
            
            # Задаём начальные значения пути и индекса.
            image_path, index = "", 1
            # Индекс нужен чтобы указать с какого 
            # элемента начинаются координаты.
            
            # Блок кода ниже нужен чтобы сохранить пробелы 
            # в пути после разделения.
            
            # Перебираем все элементы, полученные после разделения.
            for i, one_line in enumerate(line):
                # Убираем запятые в элементе.
                # Если значения элемента не являются цифрами.
                if not one_line.replace(",","").isnumeric():
                    # Если путь к изображению не пустой
                    if image_path != "": 
                        # добавляем к пути пробел.
                        image_path += " "
                    
                    # Добавляем путь к изображению.
                    image_path += one_line
                
                # Если значения элемента являются цифрами,
                # то есть координатами ограничивающей рамки,
                else:
                    # Присваиваем индексу значение номера элемента
                    index = i
                    # и выходим из цикла.
                    break
                    
            # Если нет изображения с таким именем
            if not os.path.exists(image_path):
                # отображаем ошибку.
                raise KeyError("%s does not exist ... " %image_path)
            
            # Нужно ли загружать изображения в оперативную
            # память чтобы ускорить обучение.
            if LOAD_IMAGES_TO_RAM:
                # Открываем и загружаем в память изображение.
                image = cv2.imread(image_path)
            else:
                # Иначе в переменную сохраняем пустую строку.
                image = ''
            
            # Добавляем в аннотации путь к изображению, координаты 
            # ограничивающих рамок объектов и изображение.
            final_annotations.append([image_path, line[index:], image])
        
        # Возвращаем окончательные аннотации.
        return final_annotations
    
    # Чтобы объект класса можно было перебирать 
    # итерацией, определим метод __iter__.
    def __iter__(self):
        return self

    # Функция для удаления ссылок на несуществующие изображения в аннотациях.
    def Delete_bad_annotation(self, bad_annotation):
        # Отображаем строку аннотации, в которой ошибка.
        print(f'Deleting {bad_annotation} annotation line')
        # Сохраняем имя несуществующего изображения.
        bad_image_name = bad_annotation[0].split('/')[-1]

        # Открываем текстовый документ с аннототациями
        # в режиме чтения и записи.
        with open(self.annot_path, "r+") as f:
            # Считываем строки аннотаций.
            d = f.readlines()
            # Возвращаем указатель текущей  
            # позиции на начало файла.
            f.seek(0)
            
            # Перебираем считанные строки.
            for i in d:
                # Если в строке нет имени 
                # несуществующего изображения,
                if bad_image_name not in i:
                    # записываем эту строку в файл.
                    f.write(i)
            
            # Обрезаем файл аннотаций
            # (записываем все строки аннотаций
            # без строк с ошибками).
            f.truncate()
    
    # Объявим метод __next__, который возвращает 
    # следующий доступный элемент из итератора.
    # Подготовим пакет изображений и меток.
    def __next__(self):
        # Для вычислений используем центральный процессор.
        with tf.device('/cpu:0'):
            # Сохраняем список размеров выходов сети.
            # 416 // [8, 16, 32] = [52, 26, 13]
            self.output_sizes = self.input_size // self.strides

            # Выделяем память для пакета изображений
            # формой: размер_пакета, 416, 416, 3.
            batch_image = np.zeros((self.batch_size, self.input_size, self.input_size, 3),
                                    dtype=np.float32)
            
            # Выделяем память для хранения пакета полных меток
            # ограничивающих рамок маленьких объектов
            # формой: размер_пакета, 52, 52, 3, 5+num_classes.
            batch_label_sbbox = np.zeros((self.batch_size, self.output_sizes[0], self.output_sizes[0],
                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)
            # Пакет полных меток средних объектов
            # формой: размер_пакета, 26, 26, 3, 5+num_classes.
            batch_label_mbbox = np.zeros((self.batch_size, self.output_sizes[1], self.output_sizes[1],
                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)
            # Пакет полных меток больших объектов
            # формой: размер_пакета, 13, 13, 3, 5+num_classes.
            batch_label_lbbox = np.zeros((self.batch_size, self.output_sizes[2], self.output_sizes[2],
                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)

            # Резервируем память для хранения пакета ограничивающих
            # рамок объектов разного размера формой:
            # (размер_пакета, макс_кол-во_рамок_на_выход, 4 значения xywh).
            # В нашем случае: (batch_size, 100, 4).
            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)
            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)
            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)
            
            # Флаг появления исключений (ошибок).
            exceptions = False
            
            # Счётчик заполенения пакета
            # изображениями и метками.
            num = 0 
            
            # Если текущее количество пакетов меньше 
            # максимального количества пакетов.
            if self.batch_count < self.num_batchs:
                
                # Пока пакет не заполнен (количество пройденых
                # изображений меньше размера пакета).
                while num < self.batch_size:
                    
                    # Расчитываем текущий индекс шага процесса обучения.
                    # Индекс = текущее значение счётчика пакетов * количество
                    # пакетов и + заполненость текущего пакета.
                    index = self.batch_count * self.batch_size + num
                    
                    # Если индекс больше количества образцов,
                    if index >= self.num_samples: 
                        # тогда от значения индекса отнимаем количество
                        # образцов, чтобы не выйти за допустимую область.
                        index -= self.num_samples
                    
                    # Выбираем аннотацию по индексу и сохраняем.
                    annotation = self.annotations[index]
                    # По аннотации получаем предобработанные 
                    # изображение и ограничивающие прямоугольники.
                    image, bboxes = self.parse_annotation(annotation)
                    
                    # Попробуем преобразовать рамки из аннотаций в виде 
                    # координат в список массивов меток с сетками 
                    # разных размеров и якорями для обучения.
                    # Эти преобразованные значения понадобятся 
                    # для расчёта функции потерь.
                    try:
                        label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)
                    
                    # Если возникнет ошибка при доступе
                    # (индекс вне диапазона допустимых значений), 
                    # то удаляем строку аннотации с ошибкой с файла меток.
                    except IndexError:
                        # Устанавливаем флаг исключения, что произошла ошибка.
                        exceptions = True
                        # Вызываем функцию для удаления проблемной строки меток.
                        self.Delete_bad_annotation(annotation)
                        # Выводим об этом сообщение.
                        print("Возникла проблема с изображением по такому пути", annotation[0], "Удаляем эту строку из файла аннотаций")

                    # Записываем изображение в пакет.
                    batch_image[num, :, :, :] = image
                    # Записываем полные метки в пакет 
                    # для каждого размера.
                    batch_label_sbbox[num, :, :, :, :] = label_sbbox
                    batch_label_mbbox[num, :, :, :, :] = label_mbbox
                    batch_label_lbbox[num, :, :, :, :] = label_lbbox
                    # Записываем ограничивающие рамки в пакет.
                    batch_sbboxes[num, :, :] = sbboxes
                    batch_mbboxes[num, :, :] = mbboxes
                    batch_lbboxes[num, :, :] = lbboxes
                    
                    # Увеличиваем счётчик пройденых 
                    # изображений внутри пакета.
                    num += 1

                # Если флаг ошибки установился.
                if exceptions: 
                    # Выводим сообщение о том, что была ошибка.
                    print('\n')
                    raise Exception("Произошли проблемы с набором данных. Они были устранены. Теперь нужно перезапустить процесс обучения.")
                    
                # Прибавляем к счётчику пакетов ещё один.    
                self.batch_count += 1
                # Объединяем полные метки и метки только ограничивающих рамок
                # в кортежи по размеру: маленькие, средние и большие.
                batch_smaller_target = batch_label_sbbox, batch_sbboxes
                batch_medium_target  = batch_label_mbbox, batch_mbboxes
                batch_larger_target  = batch_label_lbbox, batch_lbboxes

                # Возвращаем пакет изображений и кортеж с метками
                # для трёх размеров выходов сети.
                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)
            
            # Если мы перебрали все пакеты, 
            else:
                # то обнуляем счётчик пакетов и
                self.batch_count = 0
                # перемешиваем строки аннотаций.
                np.random.shuffle(self.annotations)
                # Этой строкой мы сообщаем итератору, 
                # что больше не нужно ничего возвращать.
                raise StopIteration
                
    # Функция отражения по горизонтали
    # изображения и ограничивающих рамок.
    def random_horizontal_flip(self, image, bboxes):
        # В половине случаев производим отражение.
        if random.random() < 0.5:
            # Определяем ширину изображения.
            _, w, _ = image.shape
            # Отражаем изображение по горизонтали.
            image = image[:, ::-1, :]
            # Таким же образом преобразовываем координаты
            # ограничивающих рамок после отражения.
            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]
        
        # Возвращаем отражённые данные.
        return image, bboxes

    # Функция случайного обрезания изображения.
    def random_crop(self, image, bboxes):
        # В половине случаев производим обрезание.
        if random.random() < 0.5:
            # Определяем высоту и ширину изображения.
            h, w, _ = image.shape
            
            # Вычисляем минимальные и максимальные координаты 
            # среди ограничивающих рамок, чтобы не обрезать объекты.
            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), 
                                       np.max(bboxes[:, 2:4], axis=0)], axis=-1)
            
            # Вычисляем допустимые границы (расстояние
            # от границы до пределов изображения).
            max_l_trans = max_bbox[0]     # левая граница
            max_u_trans = max_bbox[1]     # верхняя граница
            max_r_trans = w - max_bbox[2] # правая граница
            max_d_trans = h - max_bbox[3] # нижняя граница  
            
            # Расчитываем случайные границы в пределах допустимых.
            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans))) # левая граница
            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans))) # верхняя граница
            crop_xmax = min(w, int(max_bbox[2] + random.uniform(0, max_r_trans))) # правая граница
            crop_ymax = min(h, int(max_bbox[3] + random.uniform(0, max_d_trans))) # нижняя граница

            # Вырезаем изображение по случайным границам.
            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]

            # Корректируем координаты рамок, чтобы 
            # соответствовали обрезанному изображению.
            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin
            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin

        # Возвращаем подрезанные изображение и рамки.
        return image, bboxes

    # Функция для случайного сдвига изображения.
    def random_translate(self, image, bboxes):
        # В половине случаев производим преобразование.
        if random.random() < 0.5:
            # Определяем высоту и ширину изображения.
            h, w, _ = image.shape
            
            # Вычисляем минимальные и максимальные координаты среди 
            # ограничивающих рамок, чтобы не обрезать объекты при сдвиге.
            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)
            
            # Вычисляем допустимые границы (расстояние
            # от границы до пределов изображения).
            max_l_trans = max_bbox[0]     # левая граница
            max_u_trans = max_bbox[1]     # верхняя граница
            max_r_trans = w - max_bbox[2] # правая граница
            max_d_trans = h - max_bbox[3] # нижняя граница

            # Генерируем случайное значение сдвига в диапазоне допуcтимых границ вдоль оси х и y.
            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))
            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))

            # Формируем матрицу преобразования.
            M = np.array([[1, 0, tx], [0, 1, ty]])
            
            # С помощью аффинного преобразования сдвигаем
            # изображение в случайном направлении.
            image = cv2.warpAffine(image, M, (w, h))

            # Таким же образом сдвигаем ограничивающие рамки.
            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx
            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty

        # Возвращаем сдвинутые изображение и рамки.
        return image, bboxes
    
    # Функция для преобразования аннотации в
    # предобработанные изображение и ограничивающие рамки.
    def parse_annotation(self, annotation):
        # Если загрузили изображения в операивную память,
        if LOAD_IMAGES_TO_RAM:
            # берём путь из аннотации
            image_path = annotation[0]
            # и сохраняем ссылку на загруженное изображение.
            image = annotation[2]
            
        # Если не загружали в память,
        else:
            # берём путь из аннотации,
            image_path = annotation[0]
            # открываем и загружаем изображение.
            image = cv2.imread(image_path)
            
        # Разделяем координаты рамки и преобразовываем 
        # из текстового представления в numpy массив.
        bboxes = np.array([list(map(int, box.split(','))) for box in annotation[1]])

        # Если нужно, выполним аугментацию данных.
        if self.data_aug:
            # Выполняем горизонтальное отражение.
            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))
            # Случайно обрезаем изображение.
            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))
            # Сдвигаем изображение в случайном направлении.
            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))
        
        # Производим предобработку изображения и ограничивающих прямоугольников.
        image, bboxes = image_preprocess(np.copy(image), [self.input_size, self.input_size], 
                                         np.copy(bboxes))
        # Возвращаем обработанные изображение и рамки.
        return image, bboxes

    # Функция для подготовки правильных меток 
    # ограничивающих рамок для обучения.
    def preprocess_true_boxes(self, bboxes):
        # Создаём список из трёх numpy массивов для хранения меток, 
        # который по форме совпадает с декодированным выходом сети.
        # [[52, 52, 3, 5+num_classes],
        #  [26, 26, 3, 5+num_classes],
        #  [13, 13, 3, 5+num_classes]]
        # По сути создаём список из трёх разделительных сеток c якорями.
        label = [np.zeros((self.output_sizes[i], self.output_sizes[i], self.anchor_per_scale,
                           5 + self.num_classes)) for i in range(3)]
                           
        # Также зарезервируем массив для хранения ограничивающих рамок объектов.
        # Это будет список из трёх массивов numpy формами:
        # максимальное количество ограничивающих рамок и (x y w h),
        # в нашем случае 3 x 100 х 4.
        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]
        
        # Создадим массив для подсчёта количества рамок на каждый выход сети.
        bbox_count = np.zeros((3,))

        # Перебираем ограничивающие рамки.
        for bbox in bboxes:
            # Сохраняем координаты рамки
            bbox_coor = bbox[:4]
            # и индекс класса объекта.
            bbox_class_ind = bbox[4]

            # Перевеодим метку класса объекта в формат one hot encoding.
            # Создаём массив нулей размером равным количеству классов
            onehot = np.zeros(self.num_classes, dtype=np.float)
            # и по индексу класса записываем 1 в этот массив.
            onehot[bbox_class_ind] = 1.0
            
            # Создаём массив размером равным количеству классов и 
            # заполняем его единица делённая на количество классов.
            # В нашем случае 1/1 = 1. Заполняем еденицами.
            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)
            
            # Сглаживаем метки в формате OHE, чтобы у нас были 
            # не 0 и 1, а 0.99 где есть класс и 0.01 где нет.
            deta = 0.01 # Значение вместо 0, где нет объекта этого класса.
            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution

            # Переводим формат ограничивающих рамок
            # из координат границ в центр, ширину и высоту.
            # xmin ymin xmax ymax -> x y w h.
            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, 
                                         bbox_coor[2:] - bbox_coor[:2]], axis=-1)
                                         
            # Масштабируем ограничивающую рамку под каждый выход сети
            # относительно размера ячейки сетки выхода.
            # x y w h / [8, 16, 32]
            # Например, для объекта с координатами x y w h
            # [208, 208, 40, 80] / 8  = [26,  26,  5,    10 ]
            # [208, 208, 40, 80] / 16 = [13,  13,  2.5,  5  ]
            # [208, 208, 40, 80] / 32 = [6.5, 6.5, 1.25, 2.5]
            # То есть мы переводим из координат по пикселям
            # в координаты по разделительной сетке изображения.
            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]

            # Создаём пустой общий список коэффициентов пересечений.
            iou = []
            # Создадим флаг есть ли необходимое пересечение.
            exist_positive = False
            # Перебираем номера выходов сети.
            for i in range(3):
                # Создаём массив для хранения якорей размером 3 на 4
                # (количество якорей на выход сети и x y w h якорей).
                anchors_xywh = np.zeros((self.anchor_per_scale, 4))
                
                # Масштабированные относительно сетки значения координат центра 
                # ограничивающей рамки округляем до наименьшего целого числа и + 0.5, 
                # чтобы центр объекта совпадал с центром ячейки сетки.
                # Записываем в пустой массив якорей.
                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5
                # В места в массиве для ширины и высоты
                # записываем ширину и высоту якорей.
                anchors_xywh[:, 2:4] = self.anchors[i]

                # Вычисляем коэффициенты пересечения масштабированных 
                # ограничивающих рамок с якорями для каждого выхода сети.
                iou_scale = bbox_xywh_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)
                # Добавляем их в общий список коэффициентов пересечений.
                iou.append(iou_scale)
                
                # Создадим маску с условием, чтобы выбрать рамки 
                # с коэффициентом пересечения больше 0.3
                iou_mask = iou_scale > 0.3

                # Если хотя бы один коэффициент подпадает под условие маски.
                if np.any(iou_mask):
                    # Вычисляем индексы ячейки сетки.
                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)
                
                    # Записываем в метку по номеру выхода сети
                    # и только в те ячейки, у которых iou > 0.3.
                    # Присваиваем нули всем значениям.
                    label[i][yind, xind, iou_mask, :] = 0
                    # В место для координат ограничивающей рамки записываем 
                    # координаты прямоугольника объекта в формате x y w h.
                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh
                    # В место для показателя уверенности что
                    # в рамке есть объект записываем 1.
                    label[i][yind, xind, iou_mask, 4:5] = 1.0
                    # В место для вероятности классов объектов записываем 
                    # сглаженные метки классов в формате smooth OHE.
                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot
                    
                    # Ограничиваем индекс рамки максимальным допустимым количеством.
                    # В нашем случаем не больше 100 на каждый выход.
                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)
                    # Записываем координаты ограничивающей рамки по индексу.
                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh
                    # Прибавляем к счётчику записанных рамок ещё одну.
                    bbox_count[i] += 1

                    # Устанавливаем флаг, что есть необходимое пересечение.
                    exist_positive = True
            
            # Если всё таки нет рамок с коэф. пересечения > 0.3.
            if not exist_positive:
                
                # Находим индекс с максимальным коэф. пересечения среди всех.
                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)
                
                # Делим этот индекс на количество якорей на выход сети (/3)
                # и оставляем целую часть, чтобы получить номер выхода.
                best_detect = int(best_anchor_ind / self.anchor_per_scale)
                # Берём целую часть от остатка от деления индекса на количество 
                # якорей на выход сети (%3), чтобы получить номер якоря.
                best_anchor = int(best_anchor_ind % self.anchor_per_scale)
                
                # Вычисляем индексы ячейки сетки с наилучшим пересечением.
                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)

                # Записываем в метку по лучшему выходу сети по пересечению
                # и только в ячейку с наилучшим пересечением с якорем.
                # Присваиваем нули всем значениям.
                label[best_detect][yind, xind, best_anchor, :] = 0
                # В место для координат ограничивающей рамки записываем 
                # координаты прямоугольника объекта в формате x y w h.
                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh
                # В место для показателя уверенности что
                # в рамке есть объект записываем 1.
                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0
                # В место для вероятности классов объектов записываем 
                # сглаженные метки классов в формате smooth OHE.
                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot

                # Ограничиваем индекс рамки максимальным допустимым количеством.
                # В нашем случаем не больше 100 на каждый выход.
                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)
                # Записываем координаты ограничивающей рамки 
                # по лучшему выходу сети по пересечению.
                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh
                # Прибавляем к счётчику записанных рамок ещё одну.
                bbox_count[best_detect] += 1
        
        # Разделяем и сохраняем полные метки и ограничивающие 
        # прямоугольники по размерам выходов сети.
        # Маленькие, средние и большие объекты с рамками.
        label_sbbox, label_mbbox, label_lbbox = label
        sbboxes, mbboxes, lbboxes = bboxes_xywh
        
        # Возвращаем полные метки и огр.рамки по трём размерам.
        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes
    
    # Чтобы можно было получить длинну объекта 
    # при запросе, определим метод __len__.
    def __len__(self):
        # Возвращаем количество пакетов.
        return self.num_batchs

import os
# Если у вас несколько gpu, здесь вы можете 
# какой именно использовать для обучения.
# В нашем случае первое устройство gpu.
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
# Подтверждаем увеличение выделяемой памяти gpu.
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
# Выведем список доступных локальных устройств 
# для вычислений и информацию о них.
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

import shutil
import numpy as np
import tensorflow as tf




# Функция для обучения модели.
def main():
    # Обучение с контрольной точки.
    # Делаем переменную глобальной, 
    # чтобы сохранилось изменение значения.
    global FROM_CHECKPOINT
    
    # Получим список физических устройств, 
    # которые доступны среде выполнения.
    gpus = tf.config.experimental.list_physical_devices('GPU')
    # Отобразим список устройств gpu.
    print(f'GPUs {gpus}')
    
    # Если у нас есть хотя бы одно утройство gpu,
    if len(gpus) > 0:
        try: 
            # попробуем установить увеличение выделяемой 
            # памяти на первом устройстве из списка.
            tf.config.experimental.set_memory_growth(gpus[0], True)
        
        # Если произойдёт ошибка, игнорируем её.
        except RuntimeError: 
            pass

    # Если уже существует папка 
    # для записи логов обучения,
    if os.path.exists(LOGDIR): 
        # то очищаем её полностью.
        shutil.rmtree(LOGDIR)
        
    # Создадим средство для записи 
    # информации по процессу обучения.
    writer = tf.summary.create_file_writer(LOGDIR)

    # Создадим экземпляры классов наборов 
    # данных для обучения и проверки.
    trainset = Dataset('train')
    testset = Dataset('test')

    # Вычислим количество шагов 
    # обучения за одну эпоху.
    steps_per_epoch = len(trainset)
    
    # Инициализируем счётчик шагов обучения.
    # Один шаг обучения равен одному пакету.
    global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)

    # Если установлен флаг использования 
    # предобученых весов.
    if TRAIN_TRANSFER:
        # Создаём модель для загрузки весов.
        Darknet = Create_Yolo(input_size=INPUT_IMG_SIZE, CLASSES=COCO_CLASSES)
        # Загружаем оригинальные веса yolov3 с помощью своей 
        # функции для загрузки весов формата darknet.
        load_yolo_weights(Darknet,YOLO_V3_WEIGHTS)

    # Создаём основную модель со своими классами, которую будем обучать.
    yolo = Create_Yolo(input_size=INPUT_IMG_SIZE, training=True, CLASSES=CLASSES_PATH)
    
    # Если нужно продолжить обучение
    # с контрольной точки.
    if FROM_CHECKPOINT:
        try:
            # Пробуем загрузить веса контрольной точки в формате tf.
            yolo.load_weights(f"Checkpoints/{MODEL_NAME}")
        except ValueError:
            print("Формы тензоров весов контрольной точки и созданной модели несовместимы")
            # Сбрасываем флаг обучения с контрольной точки,
            # чтобы загрузить оригинальные веса.
            FROM_CHECKPOINT = False

    # Если нужно использовать предобученые веса 
    # и не будем обучать с контрольной точки.
    if TRAIN_TRANSFER and not FROM_CHECKPOINT:
        # Перебираем слои созданной модели
        # с оригинальными весами yolov3.
        for i, l in enumerate(Darknet.layers):
            # Получаем веса слоя. 
            layer_weights = l.get_weights()
            # Если слой имеет веса,
            if layer_weights != []:
                try:
                    # то пробуем загрузить полученные веса в 
                    # такой же слой по индексу основной 
                    # модели, которую будем обучать.
                    yolo.layers[i].set_weights(layer_weights)
                
                # Если форма тензоров весов не совпадает,
                except:
                    # тогда пропускаем этот слой.
                    print("skipping", yolo.layers[i].name)
    
    # Устанавливаем оптимизатор Adam и указываем скорость обучения.
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    # Определим функцию шага обучения.
    def train_step(image_data, target):
        # На вход функция требует пакет подготовленных
        # изображений и правильных меток.
        
        # Запускаем запись операций для 
        # автоматического дифференцирования.
        with tf.GradientTape() as tape:
            # Пропускаем через модель в режиме обучения 
            # пакет изображений и получаем предсказание сети.
            pred_result = yolo(image_data, training=True)
            
            # Начальные значения переменных функций 
            # потерь присваиваем нулю.
            giou_loss=conf_loss=prob_loss=0
            
            # giou_loss - это переменная для хранения значений
            # функции ошибки при предсказании ограничивающей рамки.
            # conf_loss - ... при предсказании показателя уверенности,
            # есть ли объект в ограничивающей рамке.
            # prob_loss - ошибка предсказания класса объекта.
            # total_loss - суммарная ошибка.
            
            # Перебираем номера выходов сети.
            # Три выхода для объектов разного размера.
            for i in range(3):
                # Разделяем предсказание сети на выходы свёрточной
                # части модели и декодированные выходы 
                # для трёх размеров сетки.
                conv, pred = pred_result[i*2], pred_result[i*2+1]
                
                # Расчитываем функцию ошибки исходя из значений предсказаний
                # свёрточных и декодированых выходов, и правильных меток.
                loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=CLASSES_PATH)
                # Разделяем составной результат по своим переменным
                # и прибавляем значения к предыдущим.
                giou_loss += loss_items[0]
                conf_loss += loss_items[1]
                prob_loss += loss_items[2]

            # Вычисляем значение суммарной ошибки.
            total_loss = giou_loss + conf_loss + prob_loss

            # Передаём в автоматический дифференциатор значение 
            # суммарной ошибки и обучаемые переменные модели. 
            # Получаем значения градиентов ошибки узлов модели, 
            # которые нужны для оптимизации.
            gradients = tape.gradient(total_loss, yolo.trainable_variables)
            
            # Передаём в оптимизатор полученные значения градиентов 
            # и обучаемые переменные модели, чтобы минимизировать значение
            # функции потерь путём подстраивания весов модели.
            optimizer.apply_gradients(zip(gradients, yolo.trainable_variables))
            
            # Увеличиваем счётчик шагов обучения на единицу.
            global_steps.assign_add(1)
            
            # Запишем значения функций ошибок в лог обучения.
            with writer.as_default():
                # tf.summary.scalar("lr", optimizer.lr, step=global_steps)
                tf.summary.scalar("loss/total_loss", total_loss, step=global_steps)
                tf.summary.scalar("loss/giou_loss", giou_loss, step=global_steps)
                tf.summary.scalar("loss/conf_loss", conf_loss, step=global_steps)
                tf.summary.scalar("loss/prob_loss", prob_loss, step=global_steps)
            writer.flush()
            
        # Возвращаем значения функций потерь и счётчика шагов обучения.
        return global_steps.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy() # optimizer.lr.numpy(), 

    # Создадим средство для записи 
    # информации по процессу проверки.
    validate_writer = tf.summary.create_file_writer(LOGDIR)
    
    # Определим функцию шага проверки.
    def validate_step(image_data, target):
        # На вход функция также требует пакет подготовленных
        # изображений и правильных меток.
        
        # Запускаем запись операций для 
        # автоматического дифференцирования.
        with tf.GradientTape() as tape:
            # Пропускаем через модель в режиме проверки 
            # пакет изображений и получаем предсказание сети.
            pred_result = yolo(image_data, training=False)
            
            # Начальные значения переменных функций 
            # потерь присваиваем нулю.
            giou_loss=conf_loss=prob_loss=0

            # Перебираем номера выходов сети.
            for i in range(3):
                # Разделяем предсказание сети на выходы свёрточной
                # части модели и декодированные выходы 
                # для трёх размеров сетки.
                conv, pred = pred_result[i*2], pred_result[i*2+1]
                
                # Расчитываем функцию ошибки исходя из значений предсказаний
                # свёрточных и декодированых выходов, и правильных меток.
                loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=CLASSES_PATH)
                # Разделяем составной результат по своим переменным
                # и прибавляем значения к предыдущим.
                giou_loss += loss_items[0]
                conf_loss += loss_items[1]
                prob_loss += loss_items[2]

            # Вычисляем значение суммарной ошибки.
            total_loss = giou_loss + conf_loss + prob_loss
            
        # При проверке мы не модифицируем значения весов, 
        # поэтому нам не нужно минимизировать функцию потерь. 
        # Мы просто возвращвем полученные значения ошибок.
        return giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()
        
    # Переходим к циклу обучения.
    # Задаём начальное значение функции 
    # потерь на проверочном наборе данных. 
    # Оно должно быть изначально большое.
    # Это нужно чтобы сравнивать результат модели на текущей эпохе 
    # с предыдущим и сохранять лучшую модель, если установлен
    # флаг SAVE_BEST_ONLY сохранения только лучшей модели.
    best_val_loss = 1000
    
    # Если установлен флаг SAVE_CHECKPOINT, то веса модели 
    # будут сохраняться по окончанию каждой эпохи.
    
    # Запускаем цикл обучения, 
    # равный количеству заданых эпох.
    for epoch in range(EPOCHS):
        # Перебираем пакеты с изображениями 
        # и метками обучающего набора данных.
        for image_data, target in trainset:
            # Передаём в функцию шага обучения пакет 
            # изображений и меток, и сохраняем результат.
            results = train_step(image_data, target)
            # Вычисляем значение текущего шага внутри эпохи.
            cur_step = results[0]%steps_per_epoch
            
            # Отображаем текущие значения: номер эпохи, сколько
            # пакетов пройдено, ошибки определения координат рамок, 
            # присутствия объекта в рамке и класса объекта.
            print("epoch:{:2.0f} step:{:5.0f}/{}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}".format(epoch, cur_step, steps_per_epoch, results[1], results[2], results[3], results[4]))

        # Если проверочный набор данных пустой, 
        if len(testset) == 0:
            # то выводим об этом сообщение,
            print("Проверочный набор данных пустой.")
            # сохраняем контрольную точку и продолжаем обучение.
            yolo.save_weights(os.path.join(CHECKPOINTS_FOLDER, MODEL_NAME))
            continue
        
        # Значения функций потерь в лог мы записывали 
        # по прохождении каждого пакета данных. 
        # При проверке мы будем записывать в лог и 
        # отображать средние значения по всем пакетам
        # по завершении каждой эпохи.
        
        # Присваиваем нулю начальные значения переменных 
        # счётчика шагов проверки и функций потерь.
        count, giou_val, conf_val, prob_val, total_val = 0., 0, 0, 0, 0
        
        # Перебираем пакеты с изображениями 
        # и метками обучающего набора данных.
        for image_data, target in testset:
            # Передаём в функцию шага проверки пакет 
            # изображений и меток, и сохраняем результат.
            results = validate_step(image_data, target)
            # Увеличиваем на единицу счётчик шагов проверки.
            count += 1
            
            # Разделяем составной результат по своим переменным
            # и прибавляем значения к предыдущим.
            giou_val += results[0]
            conf_val += results[1]
            prob_val += results[2]
            total_val += results[3]
            
        # Запишем средние значения функций ошибок в лог проверки.
        with validate_writer.as_default():
            tf.summary.scalar("validate_loss/total_val", total_val/count, step=epoch)
            tf.summary.scalar("validate_loss/giou_val", giou_val/count, step=epoch)
            tf.summary.scalar("validate_loss/conf_val", conf_val/count, step=epoch)
            tf.summary.scalar("validate_loss/prob_val", prob_val/count, step=epoch)
        validate_writer.flush()
            
        # Отображаем текущие средние значения функций потерь.
        print("\n\ngiou_val_loss:{:7.2f}, conf_val_loss:{:7.2f}, prob_val_loss:{:7.2f}, total_val_loss:{:7.2f}\n\n".
              format(giou_val/count, conf_val/count, prob_val/count, total_val/count))

        # Если установлен флаг сохранения после каждой 
        # эпохи и не нужно сохранять лучшую модель.
        if SAVE_CHECKPOINT and not SAVE_BEST_ONLY:
            # Формируем путь и имя файла со значением
            # функции ошибки на проверочном наборе. 
            # Веса будем хранить в формате tf, поэтому не 
            # будем указывать буквенно формат в имени.
            save_directory = os.path.join(CHECKPOINTS_FOLDER, MODEL_NAME+"_val_loss_{:7.2f}".format(total_val/count))
            # Сохраняем веса модели.
            yolo.save_weights(save_directory)
            
        # Если в настройках указали сохранять только
        # лучшую модель. Сохраняем только в том случае, 
        # если значение суммарной функции ошибки на
        # проверочном наборе меньше, чем на предыдущей эпохе.
        if SAVE_BEST_ONLY and best_val_loss > total_val/count:
            # Формируем путь и имя файла для сохранения.
            save_directory = os.path.join(CHECKPOINTS_FOLDER, MODEL_NAME)
            # Сохраняем веса модели под тем же именем (перезапиываем).
            yolo.save_weights(save_directory)
            # Вычисляем текущее лучшее значение
            # ошибки при проверке.
            best_val_loss = total_val/count
        
        # Если оба флага сброшены, то
        if not SAVE_BEST_ONLY and not SAVE_CHECKPOINT:
            # просто формируем путь и сохраняем веса.
            # Перезаписываем после каждой эпохи.
            save_directory = os.path.join(CHECKPOINTS_FOLDER, MODEL_NAME)
            yolo.save_weights(save_directory)

# Если мы запускаем файл как главную программу,
# а не импортируем как модуль, тогда
# вызываем функцию main.
if __name__ == '__main__':
  main()
# Это нужно для разделение кода, который
# будет выполнятся при вызове кода как модуля
# (при импортировании его в другой скрипт)
# и при запуске самого модуля, как отдельного файла.